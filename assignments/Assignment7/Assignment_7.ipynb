{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuLYCkGygMdT"
   },
   "source": [
    "# Assignment 7\n",
    "\n",
    "Name 1: Sharmila Upadhyaya  <br/>\n",
    "Student id 1: 7008858 <br/>\n",
    "Email 1: shup00001@stud.uni-saarland.de <br/>\n",
    "\n",
    "\n",
    "Name 2: Isidora Jeknic <br/>\n",
    "Student id 2: 7008924 <br/>\n",
    "Email 2: isje00001@stud.uni-saarland.de <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python files for exercises 1 and 2. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feU_fawmgoPp"
   },
   "source": [
    "## Exercise 1: Count-Trees (5 points)\n",
    "\n",
    "Your task is to implement a count-tree with variable maximum history. It is a memory-efficient way to store n-gram counts which can also be used to create an intuitive back-off after the tree is pruned.\n",
    "\n",
    "The tree object should support the following four operations:\n",
    "- Increment a count of a specific n-gram (even if not present)\n",
    "- Retrieve counts given the history (variable length)\n",
    "- Retrieve the conditional probability of a word given the history. (Proportion count between branches)\n",
    "- Pruning all nodes with counts less or equal to $k$\n",
    "\n",
    "**1.1 (2 points)**\n",
    "\n",
    "Make sure your implementation is correct by passing the asserts in the first code cell.\n",
    "\n",
    "**1.2 (1 point)**\n",
    "\n",
    "The next cell will incrementally add a quad-gram to the tree. Plot the perplexity of trigram language model (induced by this count tree) against the number of added n-grams. Comment on the curve shape. Smooth this language model with a zerogram distribution using a linear combination ($0.75\\times p_4 + 0.25\\times p_0$).\n",
    "\n",
    "**1.3 (1 point)**\n",
    "\n",
    "For the given range of thresholds, prune your tree and see how the threshold affects the performance. Plot the results (perplexity vs. threshold).\n",
    "\n",
    "**1.4 (1 point)**\n",
    "\n",
    "1. If you first prune with threshold $k_1$ and get tree $t_1$, then prune with $k_2$ and get $t_2$ what will be the relationship between $t_1$ and $t_2$ if $k_1 \\ge k_2$? (0.25 points)\n",
    "2. What is the memory benefit of count trees, in comparison to storing the counts as a dictionary `{n-gram:freq}`? (0.25 points)\n",
    "3. If we pruned the tree so that only the first level is preserved, what distribution could we model with this tree? (0.25 points)\n",
    "4. Pruning the count tree is said to be a dynamic way of smoothing the language model. Elaborate on how this smoothing happens. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_zKzeBzZLksw"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "\n",
    "tree = exercise_1.CountTree(n=4)\n",
    "\n",
    "assert tree.get(\"\") == 0\n",
    "tree.add(\"ABCE\")\n",
    "tree.add(\"ABCD\")\n",
    "tree.add(\"ABCD\")\n",
    "tree.add(\"QBCD\")\n",
    "tree.add(\"QQCD\")\n",
    "tree.add(\"BCDA\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"5634\")\n",
    "assert tree.get(\"ABCD\") == 2\n",
    "assert tree.get(\"ABCX\") == 0\n",
    "assert tree.get(\"BCD\") == 3\n",
    "assert tree.get(\"D\") == 4\n",
    "assert tree.get(\"CD\") == 4\n",
    "assert tree.get(\"1234\") == 5\n",
    "assert tree.get(\"5634\") == 1\n",
    "tree.prune(4)\n",
    "assert tree.get(\"ABCD\") == 4\n",
    "assert tree.get(\"XXCD\") == 4\n",
    "assert tree.get(\"D\") == 4\n",
    "assert tree.get(\"1234\") == 5\n",
    "assert tree.get(\"5634\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mZQ7BgD3Xr2j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2372e69160>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    " \n",
    "\n",
    "ngrams = []\n",
    "tree = exercise_1.CountTree(n=4)\n",
    "with open(\"data/alice_in_wonderland.txt\", \"r\") as f:\n",
    "  tokens = f.read().lower().split()\n",
    "  for i in range(len(tokens)-4):\n",
    "    ngrams.append(tuple(tokens[i:i+4]))\n",
    "\n",
    " \n",
    "\n",
    "vocab = set(tokens)\n",
    "for i,ngram in enumerate(ngrams):\n",
    "  tree.add(ngram)\n",
    "  if i % 1000 == 0:\n",
    "    plot_x.append(i)\n",
    "    plot_y.append(tree.perplexity(ngrams, vocab))\n",
    "\n",
    " \n",
    "\n",
    "plt.plot(plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QYPY_Whbooor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 10, 25, 50, 75, 100]\n",
      "[1.53345497548798, 1.4682240091559002, 1.4339499547876207, 1.408896146965079, 1.3990234201854972, 1.3671131818485824, 1.3500465298493947, 1.345035509012332, 1.3442579571729447, 1.3442579571729447]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2372d9b1c0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "for threshold in [1,2,3,4,5,10,25,50,75,100]:\n",
    "  tree.prune(threshold)\n",
    "  plot_x.append(threshold)\n",
    "  plot_y.append(tree.perplexity(ngrams, vocab))\n",
    "\n",
    "print(plot_x)\n",
    "print(plot_y)\n",
    "\n",
    "plt.plot(plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I8TWpt2f60B"
   },
   "source": [
    "# Answer 1.4 #\n",
    "\n",
    "1.4.1: $t_1$ and $t_2$ tree would be same since we prune first with $k_1$ and it is greater or equal to $k_2$, pruning with $k_2$ would make no change in the tree $t_1$\n",
    "\n",
    "1.4.2 It is both memory efficient and computationally cheap as in normal dictionary count, as done in previous exercise, histories are not saved together and have no direct association with there preceding element which makes it difficult to get history count through single data structure , rather we use multiple dictionaries.\n",
    "\n",
    "1.4.3 It will be an unigram distribution and will not be conditioned on histories.\n",
    "\n",
    "1.4.4 Pruning in the above exercise is smoothing in the sense that by filtering subtree with count less than equal to threshold, we are not removing those subtree. Instead we are using backing off approach by preserving the value of parent of the subtree and removing the childrens. Hence, if a ngram pair is not in the corpus, we still get the count of one element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC_TytVNlgtr"
   },
   "source": [
    "## Exercise 2: Kneser-Ney Smoothing (5 points)\n",
    "\n",
    "This exercise aims to provide a basic understanding of Kneser-Ney Smoothing. Kneser-Ney Smoothing makes use of *continuation counts* of words for lower order n-grams, given as\n",
    "\n",
    "\\begin{equation}\n",
    "C_{KN} = \n",
    "\\begin{cases}\n",
    "\\text{count}(\\bullet) & \\text{for highest order} \\\\\n",
    "\\text{continuationcount}(\\bullet) & \\text{for lower orders}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For a trigram distribution, Kneser-Ney Smoothing is implemented using the following equations:\n",
    "\n",
    "$$P_{KN}(w_3|w_1, w_2) = \\frac{\\max\\{N(w_1 w_2 w_3)-d,0\\}}{N(w_1 w_2)} + \\lambda(w_1, w_2)P_{KN}(w_3|w_2)$$\n",
    "\n",
    "$$P_{KN}(w_3|w_2) = \\frac{\\max\\{N_{+}(\\bullet w_2 w_3)-d,0\\}}{N_{+}(\\bullet w_2 \\bullet)} + \\lambda(w_2)P_{KN}(w_3)$$\n",
    "\n",
    "\\begin{equation}\n",
    "P_{KN}(w_3) = \\begin{cases}\n",
    "\\frac{N_{+}(\\bullet w_3)}{N_{+}(\\bullet \\bullet)} & \\text{if $w_3 \\in$ V} \\\\\n",
    "\\frac{1}{V} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$\\lambda$ is used to normalise the discounted probability mass and is given by\n",
    "\n",
    "$$\\lambda(w_1, w_2) = \\frac{d}{N(w_1 w_2)} \\cdot N_{+}(w_1 w_2 \\bullet)$$\n",
    "\n",
    "$$\\lambda(w_2) = \\frac{d}{N(w_2)} \\cdot N_{+}(w_2 \\bullet)$$\n",
    "\n",
    "**2.1 (4.5 points)**\n",
    "\n",
    "* Your first task is to understand what these terms represent and fill it in the table below (4-5 words each).\n",
    "\n",
    "* Create a trigram-level model on the given text, `alice_in_wonderland.txt`. Write your implementation in the file `exercise_2.py`. Preprocess the text by punctuation removal, lowercasing, and tokenisation. There is no need to split the data into train and test sets. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vNH_zLxM5amO"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)\n",
    "\n",
    "file = open(\"data/alice_in_wonderland.txt\", \"r\")\n",
    "text = file.read()\n",
    "\n",
    "# TODO: Preprocess text\n",
    "tokens = exercise_2.preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw1xFixt2rUa"
   },
   "source": [
    "* \n",
    "Write a simple class `KneserNey` in `exercise_2` that calculates the different parameters required for finding the trigram conditional probability. You may modify the function signature and add other functionality as required. <br/>\n",
    "Now, consider the trigrams `\"alice said nothing\"` and `\"alice said nichts\"`. For these trigrams, estimate the values mentioned in the table given below and fill in the obtained results. The discounting parameter *d* = 0.75. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ja0jVjrh2zu8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V| = 2574\n",
      "N+(**) = 14520\n",
      "N(w1w2w3) =  2\n",
      "N(w1w2) =  11\n",
      "N(w1w2*) = {('alice', 'said'): 6, ('said', 'nothing'): 5}\n",
      "N(w2*) - unigrams = {'said': 54, 'nothing': 21, 'alice': 143}\n",
      "N(*w2w3) = {('alice', 'said'): 11, ('said', 'nothing'): 5}\n",
      "N(*w3) = {'said': 213, 'nothing': 22, 'alice': 150}\n",
      "N(*w2*) = {'said': 462, 'nothing': 34, 'alice': 398}\n",
      "lambda - bigrams: {('alice', 'said'): 0.40909090909090906, ('said', 'nothing'): 0.625}\n",
      "lambda - unigrams: {'said': 0.08766233766233766, 'nothing': 0.4632352941176471, 'alice': 0.268796992481203}\n",
      "\n",
      "|V| = 2574\n",
      "N+(**) = 14520\n",
      "N(w1w2w3) =  0\n",
      "N(w1w2) =  11\n",
      "N(w1w2*) = {('alice', 'said'): 6, ('said', 'nichts'): 0}\n",
      "N(w2*) - unigrams = {'said': 54, 'nichts': 0, 'alice': 143}\n",
      "N(*w2w3) = {('alice', 'said'): 11, ('said', 'nichts'): 0}\n",
      "N(*w3) = {'said': 213, 'nichts': 0, 'alice': 150}\n",
      "N(*w2*) = {'said': 462, 'nichts': 0, 'alice': 398}\n",
      "lambda - bigrams: {('alice', 'said'): 0.40909090909090906, ('said', 'nichts'): 0}\n",
      "lambda - unigrams: {'said': 0.08766233766233766, 'nichts': 0, 'alice': 0.268796992481203}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KN_model = exercise_2.KneserNey(tokens, d=0.75, N=3)\n",
    "\n",
    "t1 = \"alice said nothing\"\n",
    "t2 = \"alice said nichts\"\n",
    "\n",
    "# TODO\n",
    "# Get the required parameters\n",
    "KN_model.get_params(t1)\n",
    "KN_model.get_params(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4WFNsNc5YTq"
   },
   "source": [
    "| Term in Kneser-Ney |   Value t1  | Value t2 | Description | \n",
    "|---|---|---| --- |\n",
    "|$N(w_1w_2w_3)$| 2 | 0 |Count of trigram|\n",
    "|$N(w_1 w_2)$| 11 | 11 |Count of bigram|\n",
    "|$N_{+}( \\bullet w_2 w_3)$| 5  | 0 | Number of unique trigrams ending with $(w_2, w_3)$|\n",
    "|$N_{+}( \\bullet w_2 \\bullet)$| 462 | 462 | Number of unique trigrams with $w_2$ in the second position|\n",
    "|$N_{+}( \\bullet w_3)$| 22 | 0 | Number of unique bigrams ending with $w_3$|\n",
    "|$N_{+}( \\bullet \\bullet)$| 14520 | 14520 | Number of unique bigrams |\n",
    "|$N_{+}(w_1 w_2 \\bullet)$| 6 |6 |Number of words that follow the bigram $(w_1, w_2)$ |\n",
    "|$N_{+}(w_2 \\bullet)$| 54 | 54 |Number of unique words that follow $w_2$ in a bigram|\n",
    "|$\\lambda(w_1 w_2)$| 0.40909090909090906 | 0.40909090909090906 |Normalized constant; probability mass that's been discounted (bigram)|\n",
    "|$\\lambda(w_2)$| 0.08766233766233766 | 0.08766233766233766 |Normalized constant; probability mass that's been discounted (unigram)|\n",
    "\n",
    "* Using the values obtained above, manually calculate $P_{KN}(w_3)$, $P_{KN}(w_3|w_2)$, and $P_{KN}(w_3|w_1, w_2)$ for the given trigrams. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCAmN3RMUfsX"
   },
   "source": [
    "| $P_{Kneser-Ney}$  | $t_1$ (\"alice said nothing\")  | $t_2$ (\"alice said nicht\") |  \n",
    "|-------|-------| --- |\n",
    "| $P_{KN}(w_3)$     | 0.00152 | 0.00039 |  \n",
    "|$P_{KN}(w_3|w_2)$ | 0.0093 | 3.40569e-05 |  \n",
    "|$P_{KN}(w_3|w_1, w_2)$ | 0.11745 | 1.393235e-05|\n",
    "\n",
    "\n",
    "$t_1$:  \n",
    "$ P_{KN}(w_3) = \\frac{22}{14520} = 0.0015151515151515152$\n",
    "$ P_{KN}(w_3|w_2) = \\frac{max(5-0.75, 0)}{462} + 0.08766233766233766 \\times 0.0015151515151515152 = 0.009331955922865015$\n",
    "$P_{KN}(w_3|w_1, w_2) = \\frac{max(2-0.75, 0)}{11} + 0.40909090909090906\\times0.009331955922865015 = 0.11745398196844478$  <br><br>\n",
    "\n",
    "\n",
    "$t_2$:  \n",
    "$ P_{KN}(w_3) = \\frac{1}{2574} = 0.0003885003885003885$  \n",
    "$ P_{KN}(w_3|w_2) = \\frac{max(0-0.75,0}{462} + 0.08766233766233766 \\times 0.0003885003885003885 = 3.405685223867042e-05 $  \n",
    "$P_{KN}(w_3|w_1, w_2) = \\frac{max(0-0.75,0}{11} + 0.40909090909090906 \\times 3.405685223867042e-05 = 1.3932348643092445e-05 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmr1mib2Uanq"
   },
   "source": [
    "\n",
    "**2.2 (0.5 points)**\n",
    "\n",
    "Take a look at this [video](https://www.youtube.com/watch?v=cbAxvpBFyNU) on Kneser-Ney smoothing by Dan Jurafksy. Make sure to undestand his *San Francisco* example. <br/>\n",
    "How will Kneser-Ney Smoothing handle the following bigrams (answer in 3-4 sentences)? \n",
    "\n",
    "* Abu Dhabi\n",
    "\n",
    "* Game Over\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLfMKdWBUXjy"
   },
   "source": [
    "### **Answer - 2.2**\n",
    "\n",
    "The results depend on the training corpus; if we assume that both $w_2$'s of these bigrams appear very frequently (like the *San Francisco* example), Kneser-Ney would mark *Dhabi* similarly to *Francisco*, as it only appears after *Abu*; on the other hand, *over* most likely appears in many different contexts (again, depending on the text), which Kneser-Ney would account for and adjust the probabilities accordingly. Lower unigram continuation P would be assigned to the former (*Dhabi*) than the latter (*over*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq7Yc1CmKdLi"
   },
   "source": [
    "## Bonus (2 points)\n",
    "\n",
    "For each of the smoothing techniques below,\n",
    "\n",
    "1. Laplace/add-1 smoothing (0.3 points)\n",
    "2. Add-$\\alpha$ smoothing (0.3 points)\n",
    "3. Linear interpolation (0.3 points)\n",
    "4. Absolute discounting (0.3 points)\n",
    "5. Good-Turing (0.3 points)\n",
    "6. Kneser-Ney smoothing (0.3 point)\n",
    "\n",
    "* Give the intuition behind it\n",
    "* State at least one drawback and\n",
    "* Explain how the ensuing smoothing technique accounts for this drawback.\n",
    "\n",
    "You can do so in continuous text or in bullet points. Write 3-5 sentences for each technique. For Kneser-Ney smoothing, you should suggest *and explain* an improved version from the literature, e.g. [here](http://nrs.harvard.edu/urn-3:HUL.InstRepos:25104739) (this tutorial may also be helpful for the rest of the exercise).\n",
    "\n",
    "Please note that while the points for this bonus exercise are the immediate motivation, your self-made comparison will be highly beneficial for the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSXWRyCfFuWQ"
   },
   "source": [
    "1. Laplace/add-1\n",
    "- Intuition: pretend to have seen all words one more time to avoid the zeros impacting the probability calculations.\n",
    "- Con: very blunt, makes massive changes in the reconstituted counts to account for the redistributed probability mass; not representative\n",
    "- Solution: use interpolation; additionally, use used only for specific tasks (text classification) where this won't be too much of an issue) \n",
    "\n",
    "2. Add-$\\alpha$:  \n",
    "- Intuition: pretend to have seen all words $\\alpha$ more times, to avoid the zeros. (version of Laplace, i.e. Laplace is the add-$\\alpha$ for $\\alpha = 1$)\n",
    "- Con: ignores frequency of histories; not representative\n",
    "- Solution: use linear interpolation\n",
    "\n",
    "3. Linear Interpolation:\n",
    "- Intuition: if an n-gram has been seen only a few times, it helps to use a mixture of lower-order n-grams.\n",
    "- Con: it's not always necessary to use all lower-order n-grams (if the highest-order n-gram is reliable) - waste of resources \n",
    "- Solution: \n",
    "\n",
    "4. Absolute Discounting:\n",
    "- Intuition: redistribute a fixed amount of probability mass to OOV tokens by subtracting a fixed amount from each count\n",
    "- Con: the unigram probability isn't a good estimator (the *San Francisco* example - a word may be frequent but occur only in a single context)\n",
    "- Solution: use a continuation probability (Kneser-Ney smoothing)\n",
    "\n",
    "5. Good-Turing:\n",
    "- Intuition: unseen words are as likely to appear in a text as words that were seen only once - use words seen once to approximate the counts for unseen words.\n",
    "- Con: If $k$ is large enough, the technique results in a lot of 0's, which messes with the distribution\n",
    "- Solution: Set a threshold above which frequencies remain the same or use the Simple GT smoothing variant\n",
    "6. Kneser-Ney\n",
    "- Intuition: simply counting occurances is too naive; some words tend to occur together frequency; instead of counting the # of occurrances (unigram), count the context instead\n",
    "- Con: the discount parameter is static and will not result in the optimal result for all count values.\n",
    "- Solution: apply a modified KN - instead of a fixed discounting parameter, apply a different one depending on the count (if the count is 0, 1, 2 or 3+), since the ideal discounting parameter will be different for OOV tokens, tokens that have appeared only once, twice, or three or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXMyiDu8gSwE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
