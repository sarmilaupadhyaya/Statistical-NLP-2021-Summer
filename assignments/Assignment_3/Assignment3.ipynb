{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgfjrzxW3l1W"
      },
      "source": [
        "# SNLP Assignment 3\n",
        "\n",
        "Name 1: Sharmila Upadhyaya <br/>\n",
        "Student id 1: <br/>\n",
        "Email 1: shup00001@stud.uni-saarland.de <br/>\n",
        "\n",
        "\n",
        "Name 2: Isidora Jeknic <br/>\n",
        "Student id 2: 7008924 <br/>\n",
        "Email 2: isje00001@stud.uni-saarland.de <br/> \n",
        "\n",
        "**Instructions:** Read each question carefully. <br/>\n",
        "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python file for the bonus question (if you attempt it). <br/>\n",
        "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0Looy74Lor"
      },
      "source": [
        "## Exercise 1: Entropy Intuition (2 points)\n",
        "\n",
        "### 1.1 (0.5 points)\n",
        "\n",
        "Order the following three snippets by entropy (highest to lowest). Justify your answer (view it more intuitively rather than by using a specific character-level language model, though you would probably reach the same conclusion).\n",
        "\n",
        "```\n",
        "1:    A B A A A A B B A A A B A B B B B B A\n",
        "2:    A B A B A B A B A B A B A B A B A B A\n",
        "3:    A B A A A B A B A B A B A B A B A B A\n",
        "```\n",
        "\n",
        "### 1.2 (0.5 point)\n",
        "\n",
        "Words in natural language do not have the maximum entropy given the available alphabet. This creates a redundancy (e.g. the word `maximum` could be uniquely replaced by `mxmm` and everyone would still understand). If the development of natural languages leads to somewhat optimal solutions, why is it beneficial to have such redundancies in communication?\n",
        "\n",
        "If you're uncertain, please refer to this well-written article: [www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html](http://www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html).\n",
        "\n",
        "### 1.3 (1 point)\n",
        "\n",
        "1. Assume you were given a perfect language model that would always assign probability of $1$ to the next word. What would be the cross-entropy on any text? Motivate your answer with formal derivation. (0.5 points)\n",
        "2. How does cross-entropy relate to perplexity? Is there a reason why would one be preferred over the other? (0.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWcefm746kFs"
      },
      "source": [
        "Answer \n",
        "1. The order of the sequence based on entropy (Highest to Lowest) would be 1,3 and 2. The pattern in 1 is of more type. Suppose, consider two pair of character, we will have 4 different pairs. Similarly for 3 we have 3 pairs and for 2 only 1. Hence, more bits require to encode the information in sequence 1 than 2 and 3. And 3 require more bits than 2.\n",
        "\n",
        "1.2\n",
        "TODO \n",
        "\n",
        "1.3 \n",
        "1. $Cross entropy = - \\sum_{i} (F(wi)* log(p(w_i)))$\n",
        "If probability of next word is 1 then $log(p(w_i))$ = 0 and hence cross entropy would be zero.\n",
        "2. From what we know of cross-entropy we can say that cross entropy, H(W) is the average number of bits needed to encode each word. This means that the perplexity 2^H(W) or e^ H(w) is the average number of words that can be encoded using H(W) bits. We prefer perplexity for language model, because of the exponent, improvements in perplexity \"feel\" like they are more substantial than the equivalent improvement in entropy. Another is that before they started using perplexity, the complexity of a language model was reported using a simplistic branching factor measurement that is more similar to perplexity than it is to entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VG5StIq4MVW"
      },
      "source": [
        "## Exercise 2: Harry Potter and the Measure of Uncertainty (4 points)\n",
        "\n",
        "#### 2.1 (2.5 points)\n",
        "\n",
        "Harry, Hermione, and Ron are trying to save the Philosopher's Stone. To do this, they have to cross a series of hurdles to reach the room where the stone is kept. Currently, they are trapped in a chamber whose exit is blocked by fire. On a table before them are 7 potions.\n",
        "\n",
        "|P1|P2|P3|P4|P5|P6|P7|\n",
        "|---|---|---|---|---|---|---|\n",
        "\n",
        "Of these, 6 potions are poisons and only one is the antidote that will get them through the exit. Drinking the poison will not kill them, but will weaken them considerably. \n",
        "\n",
        "1. There is no way of knowing which potion is a poison and which an antidote. How many potions must they sample *on an average* to pick the antidote? (1 point)\n",
        "\n",
        "Hermione notices a scroll lying near the potions. The scroll contains an intricate riddle written by Professor Snape that will help them determine which potion is the antidote. With the help of the clues provided, Hermione cleverly deduces that each potion can be the antidote with a certain probability. \n",
        "\n",
        "|P1|P2|P3|P4|P5|P6|P7|\n",
        "|---|---|---|---|---|---|---|\n",
        "|1/16|1/4|1/64|1/2|1/64|1/32|1/8|\n",
        "\n",
        "2. In this situation, how many potions must they now sample *on an average* to pick the antidote correctly? (1 point)\n",
        "3. What is the most efficient sequence of potions they must sample to discover the antidote? Why do you claim that in terms of how uncertain you are about guessing right? (0.5 point)\n",
        "\n",
        "#### 2.2 (1.5 points)\n",
        "\n",
        "1. Extend your logic from 2.1 to a Shannon's Game where you have to correctly guess the next word in a sentence. Assume that a word is any possible permutation and combination of 26 letters of the alphabet, and all the words have a length of at most *n*. \n",
        "How many guesses will one have to make to guess the correct word? (1 point) <br/>\n",
        "(**Hint**: Think of how many words can exist in this scenario)\n",
        "\n",
        "2. Why is the entropy lower in real-world languages? How do language models help to reduce the uncertainty of guessing the correct word? (2-3 sentences) (0.5 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wLiyDK6s6DZ"
      },
      "source": [
        "1. Probability of getting antidote = p = 1/7 and Probability of getting poison = q = (1=p)\n",
        "\n",
        "Hence, probability of getting antidote first time in nth trial would be: $pq^{n-1}$ \n",
        "\n",
        "Therefore, the weighted average for the number of sample $\\sum_{1}^{\\infty} npq^{n-1}$ \n",
        "\n",
        " $p\\sum_{1}^{\\infty} nq^{n-1}$ \n",
        " \n",
        " $=p/q\\sum_{1}^{\\infty} nq^{n}$, arithmetic-geometric series sum\n",
        "\n",
        " $=p/q \\frac{q}{{(q-1)}^2}$\n",
        "\n",
        " $=1/p$ = 1/1/7 = 7\n",
        " \n",
        "2.   \n",
        "3. \n",
        "\n",
        "4. There are $26^n$ possible words, meaning that in a uniform binomial distribution, there will be $26^n$ tries on average.\n",
        "5. Firstly, most words in a language are rare, and the few ones that aren't are super frequent. Moreover, language does not abide by the unigram model, as context (Markov assumption; word order) is of utmost importance. We discussed the necessity of grammar, but it seems that a subset of (relatively few) rules does exist and affects the overall entropy of a language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Pv_jqz6i3Y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bx1tnPQ4MgW"
      },
      "source": [
        "## Exercise 3: Kullback-Leibler Divergence (4 points)\n",
        "\n",
        "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
        "\\end{equation}\n",
        "\n",
        "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$. \n",
        "Answer the following questions:\n",
        "\n",
        "#### 3.1. (0.5 points)\n",
        "\n",
        "How is $D_{KL}$ related to Cross-Entropy? Derive a mathematical expression that describes the relationship. \n",
        "\n",
        "\n",
        "#### 3.2. (0.5 points)\n",
        "\n",
        "Is minimizing $D_{KL}$ the same thing as minimizing Cross-Entropy?  Support your answer using your answer to 1.\n",
        "\n",
        "<!-- 3.3. Is $D_{KL}$ a distance metric, i. e. does $D_{KL}(P\\|Q) = D_{KL}(Q\\|P)$ hold? Justify you explanation by a proof or by a numerical counterexample. (1 point) -->\n",
        "\n",
        "\n",
        "#### 3.3 (3 points)\n",
        "\n",
        "For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
        "\n",
        "$\\forall x,y,z \\in U:$\n",
        "\n",
        "1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
        "2. $d(x,y) = d(y,x)$\n",
        "3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
        "\n",
        "Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
        "For each of the three points either prove that it holds for $K_{DL}$ or show a counterexample proving why it does not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hweoqh7ns9jy"
      },
      "source": [
        "1. KL div is offset by entropy ?  \n",
        "DKL = H(p, q) - H(p)  \n",
        "((crossent formula))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJKNPY0gtBMD"
      },
      "source": [
        "2. Yes, entropy is fixed (a constant) and (relationship between DKL and CE).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1WsgxiYtEh7"
      },
      "source": [
        "3. D_KL is not a distance metric.  \n",
        "\n",
        "    1. holds;\n",
        "$log(\\frac {x}{y}) = 0, if x = y$ because $\\frac {y}{y} = 1$ and $log(1) = 0$\n",
        "    2. does not hold, as KL divergence is not symmetric.\n",
        "    3. does not hold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8zkUP3l4Mxw"
      },
      "source": [
        "## Bonus (1.5 points)\n",
        "\n",
        "1. Compute $D_{KL}(Q_1\\|P_1)$ for the following pair of sentences based on a unigram language model (word level).\n",
        "\n",
        "```\n",
        "p1: to be or not to be\n",
        "q1: to be or to be or not or to be be be\n",
        "```\n",
        "\n",
        " Do so by implementing the function `dkl` in `bonus.py`. You will also have to calculate the distributions $P_1$, $Q_1$; for this, you can either reuse your code from the last assignment or implement a new function in `bonus.py`. (1 point)\n",
        "\n",
        "2. Suppose the sentences in 1. would be replaced by the following sequences of symbols. You can imagine them to be sequences of nucleobases in a [coding](https://en.wikipedia.org/wiki/Coding_region) region of a gene in your genome.\n",
        "\n",
        "```\n",
        "p2: ACTGACACTGAC\n",
        "q2: ACTACTGACCCACTACTGACCC\n",
        "```\n",
        "\n",
        "Let $P_2$, $Q_2$ be the character-level unigram LMs derived from these sequences. What values will $D_{KL}(P_1\\|P_2)$, $D_{KL}(Q_1\\|Q_2)$ take? Does the quantity hold any information? Would computing $D_{KL}$ between distributions over two different natural languages hold any information? (0.5 points)\n",
        "\n",
        "No mathematical explanation nor coding required for the second part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fBOuBNr6FY8"
      },
      "source": [
        "# 1\n",
        "\n",
        "from importlib import reload\n",
        "import bonus\n",
        "bonus = reload(bonus)\n",
        "\n",
        "p1 = 'to be or not to be'\n",
        "toks_p1 = p1.split()\n",
        "\n",
        "q1 = 'to be or to be or not or to be be be'\n",
        "toks_q1 = q1.split()\n",
        "\n",
        "# TODO: estimate LMs\n",
        "P = bonus.find_ngram_probs(toks_p1)\n",
        "Q = bonus.find_ngram_probs(toks_q1)\n",
        "\n",
        "# TODO: DKL\n",
        "print(bonus.dkl(P,Q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZsRKndmAVDs"
      },
      "source": [
        "2. The value does not hold any meaning, since the units are not comparable - $P1$ and $Q1$ are sets of words that do not refer to the same thing as the characters in $P2$ and $Q2$ (nucleobases in a coding region of a gene). It just so happens that in this particular case the number of unique characters in 2 coincides with the number of unique words in 1. If we presume that each character in 2 substitutes a unique word from 1, the calculation would make sense, as the distributions in $P2$ and $Q2$ would just be different probability distributions (akin to $Q1$, a hypothetical $X1$ and $Y1$). Therefore, if these are two different languages, the information acquired by computing the $D_{KL}(L1 || L2)$ would uncover certain features about the languages' usage of the specified linguistic units. However, languages differ in features and there is rarely (never) a 1:1 mapping accross languages, so in practice it would be difficult to imagine how this method would be calculable and informative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsfGH7kAy_o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}