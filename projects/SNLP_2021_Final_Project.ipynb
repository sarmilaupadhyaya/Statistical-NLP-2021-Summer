{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SztzmhdMkrX"
   },
   "source": [
    "# SNLP 2021 Final Project\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Your final submission should contain _a separate Notebook_ (not including our instructions). You may also implement your code in different Python files and show the output in the Notebooks. In this case, make sure you submit your Python files as well. Do not submit the data files and any other debug output. Your submission should have a clear structure and should be easy to follow. Other instructions to follow have been given in detail with the project problem statement. Read them carefully.\n",
    "\n",
    "Upload the zipped folder in Teams. Only one member from the group should make the submission. The deadline for the project submission is August 20th, 2021 (23:59 pm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFra8Hp1GbWL"
   },
   "source": [
    "# 1) Introduction\n",
    "\n",
    "As you have learned so far, OOV words are a critical issue in language models, especially for morphologically rich languages. They lead to 0 MLE estimates, and also reduce the performance of the model as the OOV rate increases. In this project, we will try to address the problem of OOV words by one of the solutions proposed in Assignment 5: *using subword representations*. Your task is to perform the following steps:\n",
    "\n",
    "1. Find appropriate subwords for a given corpus. \n",
    "2. Train a neural network model over these subwords.\n",
    "3. Use the trained model to generate a newer, larger \"corpus\". \n",
    "4. Compare the OOV rates obtained on varying sizes of the generated corpora with a baseline OOV rate over word tokens.\n",
    "5. Perform hyperparameter tuning at subword and neural network level to get the optimal OOV rates over the generated corpora. \n",
    "\n",
    "**You will repeat the above steps for two language corpora: English, and Bengali.** \n",
    "\n",
    "Each of the steps in the above outline have been described in detail in the rest of the Notebook. You will also find instructions on different libraries you will utilise for implementing the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhCKb6QwQVrY"
   },
   "source": [
    "# 2) Setup\n",
    "\n",
    "## Virtual Machine\n",
    "**This is for Windows users who don't have a Linux machine or VM.**\n",
    "\n",
    "A large part of this project involves running models that require C++ binaries and wheels that are easily installable on a Linux OS. That is why, we are providing a virtual machine with the required libraries pre-installed in a project environment.\n",
    "\n",
    "1. Download and install [VMWare Workstation Player](https://www.vmware.com/products/workstation-player/workstation-player-evaluation.html).\n",
    "After downloading, On Windows, just run the `.exe` file. On Linux you have to execute the `.bundle` file with root privileges:\n",
    "```shell\n",
    "cd path/to/bundle\n",
    "chmod a+x \n",
    "sudo ./VMware-Player*.bundle --required\n",
    "```\n",
    "\n",
    "2. Download the [appliance](https://teams.microsoft.com/_#/school/files/Lecture?threadId=19%3A55c021361fe84c79811077ddbb6ef883%40thread.tacv2&ctx=channel&context=vm&rootfolder=%252Fsites%252F127759SNLP%252FFreigegebene%2520Dokumente%252FLecture%252Fvm) (you can also go to `Teams > SNLP > Lecture > files > vm`).\n",
    "\n",
    "3. Open Virtualbox, select `File -> Import Appliance` and then point to the .ova file on your system. You can use the default settings by Virtualbox, but it's a good idea to increase the memory and core count. <br/>\n",
    "The login password is 'snlp-21'. \n",
    "\n",
    "4. All the required libraries and their dependencies have been installed to the `project` conda virtual environment. Press `ctrl+alt+t` to open a console. To edit the Python files, you may install [Visual Studio Code](https://code.visualstudio.com/docs/setup/linux), or any other editor of your choice.\n",
    "\n",
    "The details of how to use these libraries are mentioned in the respective task. Those who wish to install the libraries on their local machine are free to do so. In that case, check the links given for these requirements and install them appropriately.\n",
    "\n",
    "---\n",
    "\n",
    "**Follow the steps below ONLY if you don't use the VM provided by us!**\n",
    "\n",
    "### a: Sentencepiece\n",
    "\n",
    "You will use this library to assist you with implementing subwords. \n",
    "\n",
    "Install [sentencepiece](https://github.com/google/sentencepiece) either via pip or compile it from source.\n",
    "\n",
    "### b: RNNLM\n",
    "\n",
    "The RNNLM toolkit is used to quickly train, evaluate, and use statistical language models. It implements [RNN](https://medium.com/swlh/simple-explanation-of-recurrent-neural-network-rnn-1285749cc363)s in C++. It is possible to train RNN models without a GPU using this toolkit. \n",
    "\n",
    "1. Download rnnlm from [here](http://www.fit.vutbr.cz/~imikolov/rnnlm/) and extract the archive.\n",
    "\n",
    "2. If not already installed, install the latest C++ compiler:\n",
    "\n",
    "  ```shell\n",
    "sudo apt update && apt upgrade -y\n",
    "sudo apt install build-essential \n",
    "```\n",
    "\n",
    "  Replace the old version of the cpp compiler in the make file with the current version from your machine:\n",
    "  ```shell\n",
    "ls /usr/bin/ | grep x86_64-linux-gnu-cpp\n",
    "```\n",
    "This command should show something like this:\n",
    "```\n",
    "x86_64-linux-gnu-c++\n",
    "x86_64-linux-gnu-c++-10\n",
    "x86_64-linux-gnu-c++-9\n",
    "```\n",
    "Now, put `/usr/bin/x86_64-linux-gnu-c++-10` (rather the actual output on your machine) into the first line of the makefile:\n",
    "```\n",
    "CC = /usr/bin/x86_64-linux-gnu-g++-10\n",
    "```\n",
    "\n",
    "3. Compile by typing\n",
    "```\n",
    "make -j 2\n",
    "```\n",
    "You can replace `2` with a larger number if your system has more cores/threads available, but this shouldn't take too long.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57kVLZFSyUtH"
   },
   "source": [
    "# 3) Overview\n",
    "\n",
    "Below, we describe each of the project tasks in detail. You can either show parallel implementations of all the steps for English and Bengali, or show all the tasks for English first, and repeat the process for Bengali (we recommend the second approach). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "101NY1wwGira"
   },
   "source": [
    "## **1 Data Preparation (12 points)**\n",
    "\n",
    "We will use an English corpus that you already know from the assignments (*Alice in Wonderland*), and a Bengali corpus that is decidedly different in both context and language structure. You can find the corpora in `data/bengali_corpus.txt` and `data/alice_in_wonderland.txt`.\n",
    "\n",
    "1. Preprocess both corpora such that they can serve as the input to sentencepiece. (8 points)\n",
    "\n",
    "2. Split the preprocessed corpus into a train and a test set. The test set should comprise 20% of the corpus. Write the two sets to files `train.txt` and `test.txt` (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvyFF2fwGkxC"
   },
   "source": [
    "## **2 Subword Segmentation (16 points)**\n",
    "\n",
    "### 2.1 Introduction\n",
    "\n",
    "In this section you should break down the data into smaller units. Subwords are a very powerful tools used all around NLP which bridge the generative gap between characters and whole words.\n",
    "\n",
    "Subwords are a means for tools to address the structure of the word. Instead of having just two views of the word as a string of characters and as an atomic word unit, we get a new one in between which is similar to syllables, but based on statistical properties of the text and with a degree of adjustability:\n",
    "\n",
    "- Characters: `b e d c l o t h e s`\n",
    "- Word unit: `bedclothes`\n",
    "- Subword: `▁bed clo th es`\n",
    "\n",
    "In the given example, the word `bedclothes` was split into four subword units, which as a standalone occur much more frequently than the word itself. To determine which co-ocurring characters should form a subword, we need to use a specific algorithm. Since this algorithm adds spaces in between subwords, we need to mark the beginning of a word somehow to not lose the information about the real spaces between words (segmentation needs to be reversible). In the case of [Byte-Pair-Encoding](www.aclweb.org/anthology/P16-1162) (the algorithm we are using), this is marked by the `▁` symbol (note that this is not the same as an underscore `_`).\n",
    "\n",
    "For more information either check [our tutorial](https://github.com/zouharvi/uds-snlp-tutorial/blob/main/smoothing-1/handout.pdf) (Slide 4) or this [hands-on tutorial on BPE](https://leimao.github.io/blog/Byte-Pair-Encoding/).\n",
    "\n",
    "### 2.2 Usage\n",
    "\n",
    "In this project, you are going to be using the [SentencePiece](https://github.com/google/sentencepiece) implementation of BPE.\n",
    "\n",
    "Even though installing this library is just a single `pip3 install sentencepiece` call, the VM already comes with this dependency pre-installed. It offers both shell and Python interface.\n",
    "\n",
    "The first step is to train the model and specify the target vocabulary size. This can be done with the following call:\n",
    "\n",
    "```\n",
    "spm_train \\\n",
    "  --input=path_to_train.txt \\\n",
    "  --model_prefix=model_prefix \\\n",
    "  --vocab_size=1000 \\\n",
    "  --character_coverage=1.0 \\\n",
    "  --model_type=bpe\n",
    "```\n",
    "\n",
    "This will create a vocabulary of subwords of size **1000** based on **path_to_train.txt** and store the results in **model_prefix.vocab** and **model_prefix.model**. You are asked to change the first three parameters based on the requirements of this project. <br/>\n",
    "(Note: The character coverage for non-English languages is usually around ~0.995)\n",
    "\n",
    "The next step is to segment the original text using this model. This can be done using the following call:\n",
    "\n",
    "```\n",
    "spm_encode \\\n",
    "  --model=model_prefix.model \\\n",
    "  --output_format=piece \\\n",
    "  < path_to_train.txt \\\n",
    "  > segmented.txt\n",
    "```\n",
    "\n",
    "This will segment the file **path_to_train.txt** with the model **model_prefix.model** and store the results in **segmented.txt**.\n",
    "\n",
    "After this step, depending on the configuration, the contents of the output file should look something like this:\n",
    "\n",
    "```\n",
    "...\n",
    "▁Alice ▁was ▁beginning ▁to ▁get ▁very ▁ti red ▁of ▁sit ting ▁by ▁her ▁s is ter\n",
    "▁on ▁the ▁b an k , ▁and ▁of ▁ha ving ▁nothing ▁to ▁do : ▁once ▁or ▁tw ice ▁she ▁had\n",
    "...\n",
    "```\n",
    "\n",
    "It is also desirable to be able to go back from subword unit segmentation back to the original text (e.g. after generating text in subword units). This can be done trivially using a Python one-liner, though the safer option is to use an existing tool:\n",
    "\n",
    "```\n",
    "spm_decode \\\n",
    "  --model=model_prefix.model \\\n",
    "  --input_format=piece \\\n",
    "  < segmented.txt \\\n",
    "  > original.txt\n",
    "```\n",
    "\n",
    "This will turn the segmented input text in **segmented.txt** and retrieves the human-readable text to **original.txt** using the model **model_prefix.model**.\n",
    "\n",
    "You can run shell commands from within Jupyter Notebook directly on your local system (given that you are connected to a local Python environment). And example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTx-ofkeKFLQ",
    "outputId": "8c5fde7c-7c8a-40ef-c06e-dca2fcabd190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/snlp-project-21\n",
      "Desktop    Downloads   Music\t Public  sentencepiece\tVideos\n",
      "Documents  miniconda3  Pictures  rnnlm\t Templates\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also `cd` into another directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRODNp4CKFiZ"
   },
   "source": [
    "Alternatively, you can also check the Python API for sentencepiece model training and segmentation and utilise these in your code files.\n",
    "\n",
    "### 2.3 Task\n",
    "\n",
    "You are asked to create data for a language model based on different subword granularity, namely:\n",
    "\n",
    "1. Characters. This can be done manually but also by running BPE with the output vocabulary size being the same as the input alphabet size. (4 points)\n",
    "2. Subword Units: smaller vocabulary, closer to characters. The vocabulary size is usually in the range of 100 to 800 for English. (6 points)\n",
    "3. Subword Units: larger vocabulary, closer to words. The vocabulary size is usually in the range 1500 to 3000 for English. (6 points)\n",
    "\n",
    "In 2 and 3, try to experiment with multiple values and pick one to get the best performance. \n",
    "\n",
    "You should run this on both languages (the train part of the given data), resulting in files: `en_s1.txt`, `en_s2.txt`, `en_s3.txt`, `bn_s1.txt`, `bn_s2.txt` and `bn_s3.txt`.\n",
    "\n",
    "Take a look at these files and comment briefly on what you observe in terms of word segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTJiQ24YGqJU"
   },
   "source": [
    "## **3 LM Training (20 point)**\n",
    "\n",
    "1. Now, train 3 language models based on the corpora you created in 2.3. We will do this using the RNNLM toolkit. The RNN model is trained on the subword units you have created using SentencePiece. As with all neural models, the performance and computation times depend on the number of hidden layers, backpropagation parameter. The class size is used to implement a class-based language model. <br/>  (8 points)\n",
    "\n",
    "  You can use rnnlm to train a language model with the following command:\n",
    "  ```shell\n",
    "  /home/snlp-project-21/rnnlm/rnnlm \\\n",
    "    -train /path/to/train.txt \\\n",
    "    -valid /path/to/test.txt \\\n",
    "    -rnnlm model \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class 9999\n",
    "  ```\n",
    "  Remember that you have to prefix it with `!` to run it in the Notebook. \n",
    "\n",
    "  The `model` files will be stored in the same directory as the script. Consider creating a directory to store the models:\n",
    "  ```shell\n",
    "  !rm -rf models/rnnlm \\\n",
    "    && mkdir models/rnnlm \\\n",
    "    && cd models/rnnlm \\\n",
    "    && # call of rnnlm goes here\n",
    "    ...\n",
    "  ```\n",
    "\n",
    "2. After training, the rnnlm toolkit outputs the perplexity of the trained model. Play around with the hyperparameters of rnnlm and report a perplexity that is below the baseline from **3.1**. Use these hyperparamters to train the models you will use in **4.** and **5.**  (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ypmqiARGsaB"
   },
   "source": [
    "## **4 Text Generation (16 points)**\n",
    "\n",
    "After training our language models, we are now ready to create some artificial data! Take a look at the [basic examples](http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz) on the rnnlm site in the folder `~/simple-examples/4-data-generation/test.sh` to find out how to do that (**hint:** it's only a tiny flag!). \n",
    "\n",
    "1. For every language model trained in 3, use rnnlm to generate $k = 10^1, 10^2, 10^3, 10^4, 10^5, 10^6, 10^7$ output tokens. This means that you have to run rnnlm 7 times and change the parameter of the flag in every run. (8 points)\n",
    "\n",
    "  You can use a shell loop to achieve this:\n",
    "  ```shell\n",
    "  for i in {1,2,3,4,5,6,7}; do \\\n",
    "    echo $i; \\\n",
    "    # do something\n",
    "  done\n",
    "  ```\n",
    "  See [here](https://linuxize.com/post/bash-for-loop/) for a detailed introduction. Alternatively, you can implement this in the Python file.\n",
    "\n",
    "2. Save the generated data into text files. You may name them `10.txt`, `100.txt` and so on. Make sure they are saved to different directories for every model. (2 points)\n",
    "  You can redirect the output of a shell comand with `>`:\n",
    "  ```shell\n",
    "  echo 'I am a sample text' > test.txt\n",
    "  ```\n",
    "\n",
    "Note that `>/<` appends to the end of a file. If you want to replace the content of a file use `>>/<<`.\n",
    "\n",
    "3. Inspect `100.txt` for every model. Do you see a difference in the quality of the generated data? Why could that be? <br/>\n",
    "(Note: Your generated data will be in the form of subwords. You have to decode this back to word level to compare) (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYk9s9pZG3YG"
   },
   "source": [
    "## **5 OOV comparison (16 points)**\n",
    "\n",
    "1. Using the original corpora generated in 1., find the train and test vocabulary and determine the OOV rate at word level. Do this by decoding the RNN output, adding all the generated words to your vocabulary and measuring the OOV rate. (2 points)\n",
    "\n",
    "2. Use the generated corpora from **4.** to augment the train vocabulary. Do this $k$ times, i. e. for generated corpora of size $10^1, 10^2,...,10^7$. For each model and each $k$, calculate the OOV rate of the augmented train set against the test set. (8 points)\n",
    "\n",
    "3. For each model, plot OOV rates. What do you observe? Which of the models would you use in a practical application? (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R12ZPsL6oP7j"
   },
   "source": [
    "## **6. Analysis** (20 points)\n",
    "\n",
    "Write a succinct summary of your observations for all the tasks, what you aimed to achieve, and whether your expectations were fulfilled. What are your takeaways from this project? How do your results differ for English and Bengali? What hyperparameters do you use to optimise the OOV rates? Are there any ways you could improve your results?\n",
    "\n",
    "For this section, we will also consider the overall style and how well-written the report is. You should write the summary in the same final Notebook you submit. It should be roughly 500-800 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qNd2rK5xKS8"
   },
   "source": [
    "# 4) Grading\n",
    "\n",
    "The project comprises 25% of the final grade. The grading for this project is distributed as follows:\n",
    "\n",
    "- Data preparation (12 points)\n",
    "- Subword units (16 points)\n",
    "- LM training (20 points)\n",
    "- Text generation (16 points)\n",
    "- OOV comparison (16 points)\n",
    "- Analysis (20 points)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "101NY1wwGira",
    "yRODNp4CKFiZ"
   ],
   "name": "SNLP 2021 Final Project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}